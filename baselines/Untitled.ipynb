{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab23479-e8d8-4c92-9dbc-7b8094d03274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.naming_corrections import TABLES_V5_2_V4_RENAME_LEGACY,\\\n",
    "                                   TABLES_COLUMNS_DEFAULT_LEGACY,\\\n",
    "                                   FEATURES_NAMES_FROM_NEW_CACHE,\\\n",
    "                                   FEATURES_NAMES_FROM_PRELOADED_CACHE, FEATURE_COLUMNS_OTHERS\n",
    "\n",
    "# Define the feature names and stuff\n",
    "FEATURE_COLUMNS = FEATURES_NAMES_FROM_PRELOADED_CACHE+FEATURE_COLUMNS_OTHERS\n",
    "TABLES_V5_2_V4_RENAME = TABLES_V5_2_V4_RENAME_LEGACY\n",
    "TABLES_COLUMNS_DEFAULT = TABLES_COLUMNS_DEFAULT_LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3afe43-34b5-4f58-bb95-be860018ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(\n",
    "    data_dir: str,\n",
    "    device,\n",
    "    feature_column_names: List[str] = FEATURE_COLUMNS,\n",
    "    debug=False,\n",
    "    semi_supervised=True,\n",
    "    semi_supervised_resample_negs=None, # randomize negs\n",
    "    semi_supervised_resample_factor=None, # randomize negs factor\n",
    "    splits: List[str] = [\n",
    "        DATA_LABEL_TRAIN,\n",
    "        DATA_LABEL_VAL,\n",
    "        DATA_LABEL_TEST,\n",
    "    ], \n",
    "    scaler: Optional[StandardScaler] = None,\n",
    "    rng = np.random.default_rng(seed=1),\n",
    "    features_dir: Optional[str]=None, # extra path that we use for duckdb queries\n",
    "    refresh_cache:bool = False,\n",
    "):\n",
    "    labels_dir = \"labels\"\n",
    "    partitions_dir = \"partitions.parquet\"\n",
    "    cached_features_dir = \"cache/features\"\n",
    "    cached_edges_dir = \"cache/edges\"\n",
    "\n",
    "    assert data_dir!=None, (\"data path does not exist\")\n",
    "    assert features_dir!=None, (\"duckdb path does not exist\")\n",
    "\n",
    "    datastore = load_local_data_store(data_dir)\n",
    "    features_datastore = load_local_data_store(features_dir)\n",
    "\n",
    "    assert datastore.exists(partitions_dir), \"partitions.parquet missing\"\n",
    "    assert datastore.exists(labels_dir) or is_empty(\"labels\", datastore)==False, \"labels missing\"\n",
    "\n",
    "    print (\"Building dataset\")\n",
    "\n",
    "    # If not using cache, clear cache folder\n",
    "    global FEATURE_COLUMNS, TABLES_V5_2_V4_RENAME, TABLES_COLUMNS_DEFAULT\n",
    "    if refresh_cache:\n",
    "        reset_cache(cached_features_dir, cached_edges_dir, datastore)\n",
    "        FEATURE_COLUMNS = FEATURES_NAMES_FROM_NEW_CACHE+FEATURE_COLUMNS_OTHERS\n",
    "        from lib.naming_corrections import TABLES_V5_2_V4_RENAME, TABLES_COLUMNS_DEFAULT\n",
    "        TABLES_V5_2_V4_RENAME = TABLES_V5_2_V4_RENAME\n",
    "        TABLES_COLUMNS_DEFAULT = TABLES_COLUMNS_DEFAULT\n",
    "        print (\"Using DuckDB to generate features to cache\")\n",
    "    else:\n",
    "        print (\"Using existing cache. Verifying...\")\n",
    "\n",
    "    # read the top level partition parquet\n",
    "    df_p = pd.read_parquet(\n",
    "        datastore.open_file(partitions_dir)\n",
    "    ).reset_index(drop=True).reset_index()\n",
    "\n",
    "    if debug:\n",
    "        df_p = df_p.groupby('split').first()\n",
    "\n",
    "    # we make a copy of the y in its native int\n",
    "    # - because we need to int version for the metrics (since we want to compute in GPU)\n",
    "    # - unfortunately the loss function we are using reqiures the y values\n",
    "    # to be in float\n",
    "    def build_data(X, y, edges):\n",
    "        return Data(\n",
    "            x=torch.tensor(\n",
    "                X.astype(np.float32), \n",
    "                device=device\n",
    "            ), \n",
    "            edge_index=torch.tensor(edges.T, device=device), \n",
    "            y=torch.tensor(y, device=device).float(),\n",
    "            y_i=torch.tensor(y, device=device)\n",
    "        )\n",
    "\n",
    "    from collections import Counter\n",
    "    counters = {}\n",
    "    graph_data = {}\n",
    "    labelled = {}\n",
    "    others = {}\n",
    "    for sp, A in df_p.groupby('split'):\n",
    "        graph_data[sp] = {\n",
    "            x: None\n",
    "            for x in sorted(A['index'])\n",
    "        }\n",
    "        labelled[sp] = {\n",
    "            x: None\n",
    "            for x in sorted(A['index'])\n",
    "        }\n",
    "        counters[sp] = Counter()\n",
    "\n",
    "    # Retrieve features and edges dataframes\n",
    "    for sp in splits:\n",
    "        for p in tqdm(graph_data[sp]):\n",
    "            labels_partition_filepath = os.path.join(labels_dir, f\"labels_{p}.parquet\")\n",
    "            features_partition_filepath = os.path.join(cached_features_dir, f\"features_{p}.parquet\")\n",
    "            edges_partition_filepath = os.path.join(cached_edges_dir, f\"edges_{p}.parquet\")\n",
    "\n",
    "            # Regenerate from duckdb if conditions exist\n",
    "            if (\n",
    "                refresh_cache==True\n",
    "                or datastore.exists(features_partition_filepath)==False \n",
    "                or datastore.exists(edges_partition_filepath)==False\n",
    "            ):                \n",
    "                # it should hit only one entry\n",
    "                _partition = df_p.query(f'index == {p}').iloc[0]\n",
    "\n",
    "                # df_l :\n",
    "                # - txid (in same order as df_f)\n",
    "                # - label\n",
    "                # - node (may not really be needed, not sure why another ordering )\n",
    "                df_l = pd.read_parquet(\n",
    "                    datastore.open_file(labels_partition_filepath)\n",
    "                )\n",
    "\n",
    "                df_f, df_e = generate_from_queries(df_l, _partition, features_datastore)\n",
    "\n",
    "                # Stores generated queries to cache\n",
    "                datastore.to_features_pandas(df_f, features_partition_filepath)\n",
    "                datastore.to_features_pandas(df_e, edges_partition_filepath)\n",
    "\n",
    "    # use the default names if the cache is regenerated else we keep to the legacy names from the old cache\n",
    "\n",
    "    # If it's a train loop and there is no pre-fitted scaler\n",
    "    if (\n",
    "        (DATA_LABEL_TRAIN in splits)\n",
    "        and \n",
    "        (scaler is None)\n",
    "    ):\n",
    "        scaler = fit_scaler(graph_data[DATA_LABEL_TRAIN], cached_features_dir=cached_features_dir, datastore=datastore)            \n",
    "\n",
    "    # Convert cache into a geometric dataset\n",
    "    print(\"Loading Cached Data for Training...\")\n",
    "    for sp in splits:\n",
    "        for p in tqdm(graph_data[sp]):\n",
    "            labels_partition_filepath = os.path.join(labels_dir, f\"labels_{p}.parquet\")\n",
    "            features_partition_filepath = os.path.join(cached_features_dir, f\"features_{p}.parquet\")\n",
    "            edges_partition_filepath = os.path.join(cached_edges_dir, f\"edges_{p}.parquet\")\n",
    "\n",
    "            # df_l :\n",
    "            # - txid (in same order as df_f)\n",
    "            # - label\n",
    "            # - node (may not really be needed, not sure why another ordering )\n",
    "            df_l = pd.read_parquet(\n",
    "                datastore.open_file(labels_partition_filepath)\n",
    "            )\n",
    "\n",
    "            assert datastore.exists(features_partition_filepath), f\"Missing Features Dataframe, '{features_partition_filepath}'\"                \n",
    "            assert datastore.exists(edges_partition_filepath), f\"Missing Edges Dataframe, '{edges_partition_filepath}'\"                \n",
    "            df_f, df_e = read_from_cache(features_partition_filepath, edges_partition_filepath, datastore)\n",
    "\n",
    "            X = df_f[\n",
    "                feature_column_names\n",
    "            ].fillna(value=0.).values\n",
    "\n",
    "            if scaler:    \n",
    "                X = scaler.transform(X)\n",
    "\n",
    "            # need to ensure ordering is same\n",
    "            y = df_f[['txid']].merge(\n",
    "                df_l[['txid', 'label']],\n",
    "            )['label'].values\n",
    "\n",
    "            # uses a negative sampling strategy\n",
    "            y=augment_labels(\n",
    "                y, \n",
    "                rng, \n",
    "                semi_supervised=semi_supervised, \n",
    "                semi_supervised_resample_negs=semi_supervised_resample_negs, \n",
    "                semi_supervised_resample_factor=semi_supervised_resample_factor\n",
    "                )\n",
    "\n",
    "            # if semi sup is disabled, then this will essentially\n",
    "            # be all of them\n",
    "            labelled[sp][p], = np.where(\n",
    "                y != 2\n",
    "            )\n",
    "\n",
    "            counters[sp].update(y)\n",
    "            graph_data[sp][p] = build_data(\n",
    "                X=X,\n",
    "                y=y,\n",
    "                edges=df_e[['from', 'to']].values\n",
    "            )\n",
    "\n",
    "    for sp in splits:\n",
    "        print (sp, counters[sp])\n",
    "\n",
    "    return graph_data, labelled, scaler, feature_column_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
