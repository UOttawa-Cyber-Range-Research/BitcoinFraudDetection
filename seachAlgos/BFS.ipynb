{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f064138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "factor = 150\n",
    "data_dir = \"data/dataset\"\n",
    "save_path = \"data/datasetBFS\"\n",
    "partitions_dir = \"partitions.parquet\"\n",
    "splits = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bc5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1288a533-2faf-48a2-8b47-ca1e99676cb7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "ec54a8d6-b0cd-458d-9798-270a8a81f645",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "from pyvis.network import Network\n",
    "from lib.store import Store\n",
    "from tqdm import tqdm\n",
    "import fsspec\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "\n",
    "from lib.naming_corrections import (\n",
    "    FEATURE_COLUMNS_OTHERS,\n",
    "    FEATURES_NAMES_FROM_NEW_CACHE,\n",
    "    FEATURES_NAMES_FROM_PRELOADED_CACHE,\n",
    "    TABLES_COLUMNS_DEFAULT_LEGACY,\n",
    "    TABLES_V5_2_V4_RENAME_LEGACY,\n",
    ")\n",
    "\n",
    "FEATURE_COLUMNS = FEATURES_NAMES_FROM_PRELOADED_CACHE + FEATURE_COLUMNS_OTHERS\n",
    "TABLES_V5_2_V4_RENAME = TABLES_V5_2_V4_RENAME_LEGACY\n",
    "TABLES_COLUMNS_DEFAULT = TABLES_COLUMNS_DEFAULT_LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aa2a563-8567-472c-9086-35a620caa79d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a348c79b-3202-4ef9-8e49-872d78a5b97d",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def generic_bfs_edges(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100):\n",
    "    \"\"\"Iterate over edges in a breadth-first search.\n",
    "\n",
    "    The breadth-first search begins at `source` and enqueues the\n",
    "    neighbors of newly visited nodes specified by the `neighbors`\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : NetworkX graph\n",
    "\n",
    "    source : node\n",
    "        Starting node for the breadth-first search; this function\n",
    "        iterates over only those edges in the component reachable from\n",
    "        this node.\n",
    "\n",
    "    neighbors : function\n",
    "        A function that takes a newly visited node of the graph as input\n",
    "        and returns an *iterator* (not just a list) of nodes that are\n",
    "        neighbors of that node. If not specified, this is just the\n",
    "        ``G.neighbors`` method, but in general it can be any function\n",
    "        that returns an iterator over some or all of the neighbors of a\n",
    "        given node, in any order.\n",
    "\n",
    "    depth_limit : int, optional(default=len(G))\n",
    "        Specify the maximum search depth\n",
    "\n",
    "    sort_neighbors : function\n",
    "        A function that takes the list of neighbors of given node as input, and\n",
    "        returns an *iterator* over these neighbors but with custom ordering.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    edge\n",
    "        Edges in the breadth-first search starting from `source`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> G = nx.path_graph(3)\n",
    "    >>> print(list(nx.bfs_edges(G, 0)))\n",
    "    [(0, 1), (1, 2)]\n",
    "    >>> print(list(nx.bfs_edges(G, source=0, depth_limit=1)))\n",
    "    [(0, 1)]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This implementation is from `PADS`_, which was in the public domain\n",
    "    when it was first accessed in July, 2004.  The modifications\n",
    "    to allow depth limits are based on the Wikipedia article\n",
    "    \"`Depth-limited-search`_\".\n",
    "\n",
    "    .. _PADS: http://www.ics.uci.edu/~eppstein/PADS/BFS.py\n",
    "    .. _Depth-limited-search: https://en.wikipedia.org/wiki/Depth-limited_search\n",
    "    \"\"\"\n",
    "    if callable(sort_neighbors):\n",
    "        _neighbors = neighbors\n",
    "        neighbors = lambda node: iter(sort_neighbors(_neighbors(node)))\n",
    "    \n",
    "    ratio = {0: 0, 1: 0} # counter for class 0 and class 1\n",
    "    visited = {source} # set\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "    # neighbors(source): return a generator of source's neighbors\n",
    "    queue = deque([(source, depth_limit, neighbors(source))])\n",
    "    ratio[label_dict[source]] += 1\n",
    "    \n",
    "    edges = []\n",
    "    while queue:\n",
    "        parent, depth_now, children = queue[0]\n",
    "        try:\n",
    "            # Logic for random first seach\n",
    "            child = next(children)\n",
    "            if child not in visited:\n",
    "                edges.append((parent, child))\n",
    "                visited.add(child)\n",
    "                ratio[label_dict[child]] += 1\n",
    "                if depth_now > 1:\n",
    "                    queue.append((child, depth_now - 1, neighbors(child)))\n",
    "            try:\n",
    "                if ratio[0]/ratio[1] >= factor:\n",
    "                    return edges\n",
    "            except ZeroDivisionError as e:\n",
    "                pass\n",
    "        except StopIteration:\n",
    "            queue.popleft()\n",
    "    \n",
    "    #print('factor is not fulfiled but the traverse of weekly connected component is finished:', ratio)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59ad080-8b60-4461-aa60-54da964f9881",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "c2d8eed3-fe7f-4ad8-8e36-d0f435be37d6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def load_local_data_store(data_dir:str) -> Store:   \n",
    "    # build the store\n",
    "    store = Store(\n",
    "        base_dir=data_dir,\n",
    "        protocol='file'\n",
    "    )\n",
    "    return store\n",
    "\n",
    "datastore = load_local_data_store(data_dir)\n",
    "df_p = pd.read_parquet(\n",
    "        datastore.open_file(partitions_dir)\n",
    "    ).reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5aad7f-a19d-4e0d-b109-018e35370c7e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a7059ad3-9ed8-41c4-931a-e9f39fa5e580",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "counters = {}\n",
    "graph_data = {}\n",
    "labelled = {}\n",
    "others = {}\n",
    "for sp, A in df_p.groupby('split'):\n",
    "    graph_data[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    labelled[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    counters[sp] = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba361df",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "5301c583-2ae8-481e-ba97-67ab09de6c03",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Scaler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [01:51<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "def fit_scaler(graph_data: Dict):\n",
    "    print(\"Fitting Scaler...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    for p in tqdm(graph_data):\n",
    "        df_f = pd.read_parquet(f\"./data/dataset/cache/features/features_{p}.parquet\")\n",
    "        X = df_f[FEATURE_COLUMNS].fillna(value=0.0).values\n",
    "        scaler.partial_fit(X)\n",
    "        del df_f, X\n",
    "        gc.collect()\n",
    "    return scaler\n",
    "\n",
    "scaler = fit_scaler(graph_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe757ef-dbfa-43cc-b98c-a5cce84b000e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "7cc2be65-a784-4550-b2ce-e1581cdd3cd0",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 377/377 [39:20<00:00,  6.26s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [03:06<00:00,  3.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [02:41<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# New parition file\n",
    "p_dict = {'index': [], 'split': []}\n",
    "\n",
    "# Placeholder\n",
    "idx = 0\n",
    "\n",
    "# Loop over splits\n",
    "for sp in splits:\n",
    "\n",
    "    # Loop over split data\n",
    "    for p in tqdm(graph_data[sp]):\n",
    "        \n",
    "        # Load the required files\n",
    "        network = pd.read_parquet(f'./{data_dir}/cache/edges/edges_{p}.parquet')\n",
    "        labels = pd.read_parquet(f'./{data_dir}/labels/labels_{p}.parquet')\n",
    "        features = pd.read_parquet(f'./{data_dir}/cache/features/features_{p}.parquet')\n",
    "    \n",
    "        # Convert to network x graph\n",
    "        G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "        successors = G.neighbors # can retrieve all neighbors of a particular node with []\n",
    "        \n",
    "        # Replace all label 2 as label 0\n",
    "        labels.loc[labels['label'] == 2, 'label'] = 0\n",
    "        label_dict = dict(zip(labels.node, labels.label))\n",
    "    \n",
    "        # Construct graph from each positive node\n",
    "        for _, pos_node in enumerate(labels[labels['label']==1].node.values):\n",
    "            try: # the node in the label parquet may not exist in the edge parquet\n",
    "                result = generic_bfs_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "            # Convert the FS seach to dataframe from merging and stuff\n",
    "            df_result = pd.DataFrame(result, columns=['from', 'to'])\n",
    "            \n",
    "            # Undirected to directed\n",
    "            directed1 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['from', 'to'])\n",
    "            directed2 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['to', 'from'])[['from_x', 'to_x', 'partition']].rename(columns={\"from_x\": \"from\", \"to_x\": \"to\", 'partition': 'partition'})\n",
    "            samples = pd.concat([directed1, directed2], axis=0)\n",
    "            edges_list = samples[['from', 'to']].values\n",
    "    \n",
    "            # Get the unique graph edges\n",
    "            df_node = pd.DataFrame(set(edges_list.reshape(-1)), columns=['node'])\n",
    "    \n",
    "            # Process the features and labels\n",
    "            sample_labels = labels.merge(df_node, how='inner').sort_values(by=['node'])\n",
    "            sample_features = features.merge(sample_labels, how='inner', on='txid').sort_values(by=['node']).drop(['node'], axis=1)\n",
    "            mapping = dict(zip(sample_labels.node.values, range(len(sample_labels))))\n",
    "            samples[['from', 'to']] = samples[['from', 'to']].replace(mapping)\n",
    "            sample_labels.node = [i for i in range(len(sample_labels))]\n",
    "            sample_features[FEATURE_COLUMNS] = scaler.transform(sample_features[FEATURE_COLUMNS].values)\n",
    "    \n",
    "            # Save the data\n",
    "            samples.to_parquet(f'./{save_path}/cache/edges/edges_{idx}.parquet')\n",
    "            sample_labels.to_parquet(f'./{save_path}/labels/labels_{idx}.parquet')\n",
    "            sample_features.to_parquet(f'./{save_path}/cache/features/features_{idx}.parquet')\n",
    "    \n",
    "            # New partition deck\n",
    "            p_dict['index'].append(idx)\n",
    "            p_dict['split'].append(sp)\n",
    "    \n",
    "            # Increment the idx\n",
    "            idx += 1\n",
    "\n",
    "# Save the parition file\n",
    "pd.DataFrame.from_dict(p_dict).to_parquet(f\"./{save_path}/partitions.parquet\")"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
