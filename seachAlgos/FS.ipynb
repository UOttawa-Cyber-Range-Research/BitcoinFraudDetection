{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f064138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "factor = 150\n",
    "data_dir = \"data/dataset\"\n",
    "save_path = \"data/datasetFS\"\n",
    "partitions_dir = \"partitions.parquet\"\n",
    "splits = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bc5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1288a533-2faf-48a2-8b47-ca1e99676cb7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "ec54a8d6-b0cd-458d-9798-270a8a81f645",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "from pyvis.network import Network\n",
    "from lib.store import Store\n",
    "from tqdm import tqdm\n",
    "import fsspec\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "\n",
    "from lib.naming_corrections import (\n",
    "    FEATURE_COLUMNS_OTHERS,\n",
    "    FEATURES_NAMES_FROM_NEW_CACHE,\n",
    "    FEATURES_NAMES_FROM_PRELOADED_CACHE,\n",
    "    TABLES_COLUMNS_DEFAULT_LEGACY,\n",
    "    TABLES_V5_2_V4_RENAME_LEGACY,\n",
    ")\n",
    "\n",
    "FEATURE_COLUMNS = FEATURES_NAMES_FROM_PRELOADED_CACHE + FEATURE_COLUMNS_OTHERS\n",
    "TABLES_V5_2_V4_RENAME = TABLES_V5_2_V4_RENAME_LEGACY\n",
    "TABLES_COLUMNS_DEFAULT = TABLES_COLUMNS_DEFAULT_LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "187c84cb-1cfc-4402-b9b8-b72a5a2677ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_frontier_edges(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100):\n",
    "    \"\"\"\n",
    "    Iterate over edges in a MHR search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the ratio \n",
    "    ratio = {0: 0, 1: 0}\n",
    "    visited = {source}\n",
    "\n",
    "    # Define the depth limit\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    # Define the frontier nodes\n",
    "    frontier_nodes = set()\n",
    "    neighbors_s_node = set(neighbors(source))\n",
    "    frontier_nodes.update(neighbors_s_node)\n",
    "\n",
    "    # Update the label_dict here as well\n",
    "    for node in frontier_nodes:\n",
    "        ratio[label_dict[node]] += 1\n",
    "\n",
    "    # Maintain the ratio\n",
    "    ratio[label_dict[source]] += 1\n",
    "\n",
    "    # Placeholder for collected edges\n",
    "    edges = [(source, i) for i in frontier_nodes]\n",
    "\n",
    "    # Define the logic for MHRW\n",
    "    patience = 50\n",
    "    while (ratio[0]/ratio[1]) <= factor:\n",
    "        # Select a new node with some probability\n",
    "        degrees = [G.degree(i) for i in frontier_nodes]\n",
    "        list_probs = [G.degree(i) / sum(degrees) for i in frontier_nodes]\n",
    "        \n",
    "        # Get the next child\n",
    "        chosen_frontier = np.random.choice(list(frontier_nodes), p=list_probs, size=1)[0]\n",
    "\n",
    "        # Add the parent to the visited\n",
    "        visited.add(chosen_frontier)\n",
    "        \n",
    "        # Neighbors of child\n",
    "        frontier_neighbors = neighbors(chosen_frontier)\n",
    "\n",
    "        # Randomly chose one of them\n",
    "        random_child = random.choice(list(frontier_neighbors))\n",
    "\n",
    "        # Add nodes and update visited\n",
    "        if random_child not in visited:\n",
    "            # Update the edges and other list\n",
    "            edges.append((chosen_frontier, random_child))\n",
    "\n",
    "            # Update the visited and ratio\n",
    "            visited.add(random_child)\n",
    "            ratio[label_dict[random_child]] += 1\n",
    "\n",
    "            # Replace u by v in the node list\n",
    "            frontier_nodes.remove(chosen_frontier)\n",
    "            frontier_nodes.add(random_child)\n",
    "            print(ratio[0]/ratio[1])\n",
    "\n",
    "            # Reset patience\n",
    "            patience = 0\n",
    "        else:\n",
    "            # Increment the patience\n",
    "            patience += 1\n",
    "\n",
    "            # Edge case for unreacheable nodes\n",
    "            if patience == 50:\n",
    "                frontier_nodes = visited - set([source])\n",
    "                \n",
    "    # Return the data\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ad080-8b60-4461-aa60-54da964f9881",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "c2d8eed3-fe7f-4ad8-8e36-d0f435be37d6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def load_local_data_store(data_dir:str) -> Store:   \n",
    "    # build the store\n",
    "    store = Store(\n",
    "        base_dir=data_dir,\n",
    "        protocol='file'\n",
    "    )\n",
    "    return store\n",
    "\n",
    "datastore = load_local_data_store(data_dir)\n",
    "df_p = pd.read_parquet(\n",
    "        datastore.open_file(partitions_dir)\n",
    "    ).reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5aad7f-a19d-4e0d-b109-018e35370c7e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a7059ad3-9ed8-41c4-931a-e9f39fa5e580",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "counters = {}\n",
    "graph_data = {}\n",
    "labelled = {}\n",
    "others = {}\n",
    "for sp, A in df_p.groupby('split'):\n",
    "    graph_data[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    labelled[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    counters[sp] = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba361df",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "5301c583-2ae8-481e-ba97-67ab09de6c03",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def fit_scaler(graph_data: Dict):\n",
    "    print(\"Fitting Scaler...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    for p in tqdm(graph_data):\n",
    "        df_f = pd.read_parquet(f\"./data/dataset/cache/features/features_{p}.parquet\")\n",
    "        X = df_f[FEATURE_COLUMNS].fillna(value=0.0).values\n",
    "        scaler.partial_fit(X)\n",
    "        del df_f, X\n",
    "        gc.collect()\n",
    "    return scaler\n",
    "\n",
    "scaler = fit_scaler(graph_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe757ef-dbfa-43cc-b98c-a5cce84b000e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "7cc2be65-a784-4550-b2ce-e1581cdd3cd0",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New parition file\n",
    "p_dict = {'index': [], 'split': []}\n",
    "\n",
    "# Placeholder\n",
    "idx = 0\n",
    "\n",
    "# Loop over splits\n",
    "for sp in splits:\n",
    "\n",
    "    # Loop over split data\n",
    "    for p in tqdm(graph_data[sp]):\n",
    "        \n",
    "        # Load the required files\n",
    "        network = pd.read_parquet(f'./{data_dir}/cache/edges/edges_{p}.parquet')\n",
    "        labels = pd.read_parquet(f'./{data_dir}/labels/labels_{p}.parquet')\n",
    "        features = pd.read_parquet(f'./{data_dir}/cache/features/features_{p}.parquet')\n",
    "    \n",
    "        # Convert to network x graph\n",
    "        G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "        successors = G.neighbors # can retrieve all neighbors of a particular node with []\n",
    "        \n",
    "        # Replace all label 2 as label 0\n",
    "        labels.loc[labels['label'] == 2, 'label'] = 0\n",
    "        label_dict = dict(zip(labels.node, labels.label))\n",
    "    \n",
    "        # Construct graph from each positive node\n",
    "        for _, pos_node in enumerate(labels[labels['label']==1].node.values):\n",
    "            try: # the node in the label parquet may not exist in the edge parquet\n",
    "                result = generic_frontier_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "            # Convert the FS seach to dataframe from merging and stuff\n",
    "            df_result = pd.DataFrame(result, columns=['from', 'to'])\n",
    "            \n",
    "            # Undirected to directed\n",
    "            directed1 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['from', 'to'])\n",
    "            directed2 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['to', 'from'])[['from_x', 'to_x', 'partition']].rename(columns={\"from_x\": \"from\", \"to_x\": \"to\", 'partition': 'partition'})\n",
    "            samples = pd.concat([directed1, directed2], axis=0)\n",
    "            edges_list = samples[['from', 'to']].values\n",
    "    \n",
    "            # Get the unique graph edges\n",
    "            df_node = pd.DataFrame(set(edges_list.reshape(-1)), columns=['node'])\n",
    "    \n",
    "            # Process the features and labels\n",
    "            sample_labels = labels.merge(df_node, how='inner').sort_values(by=['node'])\n",
    "            sample_features = features.merge(sample_labels, how='inner', on='txid').sort_values(by=['node']).drop(['node'], axis=1)\n",
    "            mapping = dict(zip(sample_labels.node.values, range(len(sample_labels))))\n",
    "            samples[['from', 'to']] = samples[['from', 'to']].replace(mapping)\n",
    "            sample_labels.node = [i for i in range(len(sample_labels))]\n",
    "            sample_features[FEATURE_COLUMNS] = scaler.transform(sample_features[FEATURE_COLUMNS].values)\n",
    "    \n",
    "            # Save the data\n",
    "            samples.to_parquet(f'./{save_path}/cache/edges/edges_{idx}.parquet')\n",
    "            sample_labels.to_parquet(f'./{save_path}/labels/labels_{idx}.parquet')\n",
    "            sample_features.to_parquet(f'./{save_path}/cache/features/features_{idx}.parquet')\n",
    "    \n",
    "            # New partition deck\n",
    "            p_dict['index'].append(idx)\n",
    "            p_dict['split'].append(sp)\n",
    "    \n",
    "            # Increment the idx\n",
    "            idx += 1\n",
    "\n",
    "# Save the parition file\n",
    "pd.DataFrame.from_dict(p_dict).to_parquet(f\"./{save_path}/partitions.parquet\")"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
