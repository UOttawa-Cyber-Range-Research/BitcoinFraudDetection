{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f064138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "factor = 150\n",
    "data_dir = \"data/dataset\"\n",
    "save_path = \"data/datasetFS\"\n",
    "partitions_dir = \"partitions.parquet\"\n",
    "splits = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bc5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1288a533-2faf-48a2-8b47-ca1e99676cb7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "ec54a8d6-b0cd-458d-9798-270a8a81f645",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "from pyvis.network import Network\n",
    "from lib.store import Store\n",
    "from tqdm.notebook import tqdm\n",
    "import fsspec\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "\n",
    "from lib.naming_corrections import (\n",
    "    FEATURE_COLUMNS_OTHERS,\n",
    "    FEATURES_NAMES_FROM_NEW_CACHE,\n",
    "    FEATURES_NAMES_FROM_PRELOADED_CACHE,\n",
    "    TABLES_COLUMNS_DEFAULT_LEGACY,\n",
    "    TABLES_V5_2_V4_RENAME_LEGACY,\n",
    ")\n",
    "\n",
    "FEATURE_COLUMNS = FEATURES_NAMES_FROM_PRELOADED_CACHE + FEATURE_COLUMNS_OTHERS\n",
    "TABLES_V5_2_V4_RENAME = TABLES_V5_2_V4_RENAME_LEGACY\n",
    "TABLES_COLUMNS_DEFAULT = TABLES_COLUMNS_DEFAULT_LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187c84cb-1cfc-4402-b9b8-b72a5a2677ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_frontier_edges(G, source, label_dict, neighbors=None, depth_limit=None,\n",
    "                           sort_neighbors=None, factor=100, num_frontiers=20):\n",
    "    \"\"\"\n",
    "    Iterate over edges in a MHR search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the ratio \n",
    "    ratio = {0: 0, 1: 0}\n",
    "    visited = {source}\n",
    "    break_condition = False\n",
    "\n",
    "    # Define the depth limit\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    # Define the frontier nodes\n",
    "    frontier_nodes = set()\n",
    "    neighbors_s_node = set(neighbors(source))\n",
    "\n",
    "    # Find the reachable frontier nodes\n",
    "    reachable_nodes = list(nx.single_source_shortest_path_length(G, source).keys())\n",
    "    reachable_nodes.remove(source)\n",
    "\n",
    "    # Add some random nodes just in case\n",
    "    temp_ = list(np.random.choice(list(G.nodes), size=num_frontiers))\n",
    "    reachable_nodes.extend(temp_)\n",
    "    reachable_nodes = [i for i in reachable_nodes if i in label_dict][:num_frontiers]\n",
    "\n",
    "    # Update the frontier nodes\n",
    "    frontier_nodes.update(reachable_nodes)\n",
    "    \n",
    "    # Maintain the ratio\n",
    "    ratio[label_dict[source]] += 1\n",
    "\n",
    "    # Placeholder for collected edges\n",
    "    edges = [(source, i) for i in frontier_nodes]\n",
    "\n",
    "    # Add the seed nodes to the mix\n",
    "    for node in frontier_nodes:\n",
    "        ratio[label_dict[node]] += 1\n",
    "\n",
    "    # Define the patience\n",
    "    patience = 0\n",
    "\n",
    "    # Loop and collect edgeneric_frontier_edges\n",
    "    while (ratio[0]/ratio[1]) < factor:\n",
    "        # Select a new node with some probability\n",
    "        degrees = [G.degree(i) for i in frontier_nodes]\n",
    "        list_probs = [G.degree(i) / sum(degrees) for i in frontier_nodes]\n",
    "        \n",
    "        # Get the next child\n",
    "        chosen_frontier = np.random.choice(list(frontier_nodes), p=list_probs, size=1)[0]\n",
    "\n",
    "        # Neighbors of child\n",
    "        frontier_neighbors = list(neighbors(chosen_frontier))\n",
    "\n",
    "        # Randomly chose one of them\n",
    "        random_child = random.choice(frontier_neighbors)\n",
    "\n",
    "        # Add nodes and update visited\n",
    "        if random_child not in visited and label_dict[random_child] != 1:\n",
    "            # Update the edges and other list\n",
    "            edges.append((chosen_frontier, random_child))\n",
    "            ratio[label_dict[random_child]] += 1\n",
    "\n",
    "            # Replace u by v in the node list\n",
    "            frontier_nodes.remove(chosen_frontier)\n",
    "            frontier_nodes.add(random_child)\n",
    "\n",
    "            # Reset patience\n",
    "            patience = 0\n",
    "        else:\n",
    "            # Increment the patience\n",
    "            patience += 1\n",
    "\n",
    "            if break_condition:\n",
    "                break\n",
    "            \n",
    "            # Edge case for unreacheable nodes\n",
    "            if patience == 50:\n",
    "                frontier_nodes = visited - set([source])\n",
    "                break_condition = True\n",
    "            \n",
    "    # Return the data\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59ad080-8b60-4461-aa60-54da964f9881",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "c2d8eed3-fe7f-4ad8-8e36-d0f435be37d6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def load_local_data_store(data_dir:str) -> Store:   \n",
    "    # build the store\n",
    "    store = Store(\n",
    "        base_dir=data_dir,\n",
    "        protocol='file'\n",
    "    )\n",
    "    return store\n",
    "\n",
    "datastore = load_local_data_store(data_dir)\n",
    "df_p = pd.read_parquet(\n",
    "        datastore.open_file(partitions_dir)\n",
    "    ).reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5aad7f-a19d-4e0d-b109-018e35370c7e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a7059ad3-9ed8-41c4-931a-e9f39fa5e580",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "counters = {}\n",
    "graph_data = {}\n",
    "labelled = {}\n",
    "others = {}\n",
    "for sp, A in df_p.groupby('split'):\n",
    "    graph_data[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    labelled[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    counters[sp] = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba361df",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "5301c583-2ae8-481e-ba97-67ab09de6c03",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Scaler...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2961f53b22c4f70beb0c6bd777f4c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit_scaler(graph_data: Dict):\n",
    "    print(\"Fitting Scaler...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    for p in tqdm(graph_data):\n",
    "        df_f = pd.read_parquet(f\"./data/dataset/cache/features/features_{p}.parquet\")\n",
    "        X = df_f[FEATURE_COLUMNS].fillna(value=0.0).values\n",
    "        scaler.partial_fit(X)\n",
    "        del df_f, X\n",
    "        gc.collect()\n",
    "    return scaler\n",
    "\n",
    "scaler = fit_scaler(graph_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe757ef-dbfa-43cc-b98c-a5cce84b000e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "7cc2be65-a784-4550-b2ce-e1581cdd3cd0",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210969974b3b4f11b6adad6bb3f6db6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m sample_features[FEATURE_COLUMNS] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(sample_features[FEATURE_COLUMNS]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Save the data\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/cache/edges/edges_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m sample_labels\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/labels/labels_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m sample_features\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cache/features/features_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/core/frame.py:2836\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2752\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2832\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2833\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2834\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2837\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/io/parquet.py:420\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    418\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 420\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/io/parquet.py:200\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         \u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/io/common.py:117\u001b[0m, in \u001b[0;36mIOHandles.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles:\n\u001b[0;32m--> 117\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New parition file\n",
    "p_dict = {'index': [], 'split': []}\n",
    "\n",
    "# Placeholder\n",
    "idx = 0\n",
    "\n",
    "# Loop over splits\n",
    "for sp in splits:\n",
    "\n",
    "    # Loop over split data\n",
    "    bar = tqdm(graph_data[sp])\n",
    "    for p in bar:\n",
    "        \n",
    "        # Load the required files\n",
    "        network = pd.read_parquet(f'./{data_dir}/cache/edges/edges_{p}.parquet')\n",
    "        labels = pd.read_parquet(f'./{data_dir}/labels/labels_{p}.parquet')\n",
    "        features = pd.read_parquet(f'./{data_dir}/cache/features/features_{p}.parquet')\n",
    "    \n",
    "        # Convert to network x graph\n",
    "        G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "        successors = G.neighbors # can retrieve all neighbors of a particular node with []\n",
    "        \n",
    "        # Replace all label 2 as label 0\n",
    "        labels.loc[labels['label'] == 2, 'label'] = 0\n",
    "        label_dict = dict(zip(labels.node, labels.label))\n",
    "    \n",
    "        # Construct graph from each positive node\n",
    "        for _, pos_node in enumerate(labels[labels['label']==1].node.values):\n",
    "            try: # the node in the label parquet may not exist in the edge parquet\n",
    "                result = generic_frontier_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "            # Convert the FS seach to dataframe from merging and stuff\n",
    "            df_result = pd.DataFrame(result, columns=['from', 'to'])\n",
    "            \n",
    "            # Undirected to directed\n",
    "            directed1 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['from', 'to'])\n",
    "            directed2 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['to', 'from'])[['from_x', 'to_x', 'partition']].rename(columns={\"from_x\": \"from\", \"to_x\": \"to\", 'partition': 'partition'})\n",
    "            samples = pd.concat([directed1, directed2], axis=0)\n",
    "            edges_list = samples[['from', 'to']].values\n",
    "    \n",
    "            # Get the unique graph edges\n",
    "            df_node = pd.DataFrame(set(edges_list.reshape(-1)), columns=['node'])\n",
    "    \n",
    "            # Process the features and labels\n",
    "            sample_labels = labels.merge(df_node, how='inner').sort_values(by=['node'])\n",
    "            sample_features = features.merge(sample_labels, how='inner', on='txid').sort_values(by=['node']).drop(['node'], axis=1)\n",
    "            mapping = dict(zip(sample_labels.node.values, range(len(sample_labels))))\n",
    "            samples[['from', 'to']] = samples[['from', 'to']].replace(mapping)\n",
    "            sample_labels.node = [i for i in range(len(sample_labels))]\n",
    "            sample_features[FEATURE_COLUMNS] = scaler.transform(sample_features[FEATURE_COLUMNS].values)\n",
    "    \n",
    "            # Save the data\n",
    "            samples.to_parquet(f'./{save_path}/cache/edges/edges_{idx}.parquet')\n",
    "            sample_labels.to_parquet(f'./{save_path}/labels/labels_{idx}.parquet')\n",
    "            sample_features.to_parquet(f'./{save_path}/cache/features/features_{idx}.parquet')\n",
    "    \n",
    "            # New partition deck\n",
    "            p_dict['index'].append(idx)\n",
    "            p_dict['split'].append(sp)\n",
    "    \n",
    "            # Increment the idx\n",
    "            idx += 1\n",
    "\n",
    "            # Set the bar description\n",
    "            bar.set_description(f\"File name : {p} | Saved File Name : {idx}\")\n",
    "\n",
    "        # delete the extra stuff and clear memory\n",
    "        del network, labels, features, G, successors\n",
    "        gc.collect()\n",
    "\n",
    "# Save the parition file\n",
    "pd.DataFrame.from_dict(p_dict).to_parquet(f\"./{save_path}/partitions.parquet\")"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
