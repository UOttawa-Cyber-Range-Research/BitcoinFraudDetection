{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f064138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "factor = 150\n",
    "data_dir = \"./data/dataset\"\n",
    "save_path = \"data/datasetBF\"\n",
    "partitions_dir = \"partitions.parquet\"\n",
    "splits = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bc5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1288a533-2faf-48a2-8b47-ca1e99676cb7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "ec54a8d6-b0cd-458d-9798-270a8a81f645",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "from pyvis.network import Network\n",
    "from lib.store import Store\n",
    "from tqdm.notebook import tqdm\n",
    "import fsspec\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "\n",
    "from lib.naming_corrections import (\n",
    "    FEATURE_COLUMNS_OTHERS,\n",
    "    FEATURES_NAMES_FROM_NEW_CACHE,\n",
    "    FEATURES_NAMES_FROM_PRELOADED_CACHE,\n",
    "    TABLES_COLUMNS_DEFAULT_LEGACY,\n",
    "    TABLES_V5_2_V4_RENAME_LEGACY,\n",
    ")\n",
    "\n",
    "FEATURE_COLUMNS = FEATURES_NAMES_FROM_PRELOADED_CACHE + FEATURE_COLUMNS_OTHERS\n",
    "TABLES_V5_2_V4_RENAME = TABLES_V5_2_V4_RENAME_LEGACY\n",
    "TABLES_COLUMNS_DEFAULT = TABLES_COLUMNS_DEFAULT_LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aa2a563-8567-472c-9086-35a620caa79d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a348c79b-3202-4ef9-8e49-872d78a5b97d",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def bfs_custom(G, source, label_dict, neighbors=None, depth_limit=None, ratio=None, visited=None):\n",
    "    '''\n",
    "    Generic breath first seach algorithm\n",
    "    '''\n",
    "    # Define the depth of the search\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "        \n",
    "    # neighbors(source): return a generator of source's neighbors\n",
    "    queue = deque([(source, depth_limit, neighbors(source))])\n",
    "\n",
    "    # Placeholder for edges\n",
    "    edges = []\n",
    "\n",
    "    # Run the queue\n",
    "    while queue:\n",
    "        parent, depth_now, children = queue[0]\n",
    "        try:\n",
    "            # Logic for random first seach\n",
    "            child = next(children)\n",
    "\n",
    "            # If the child has not been visited before\n",
    "            if child not in visited:\n",
    "                # Construct an edge\n",
    "                edges.append((parent, child))\n",
    "\n",
    "                # Update visited\n",
    "                visited.add(child)\n",
    "\n",
    "                # Increment the ratio\n",
    "                ratio[label_dict[child]] += 1\n",
    "\n",
    "                # Incremen the child\n",
    "                if depth_now > 1:\n",
    "                    queue.append((child, depth_now - 1, neighbors(child)))\n",
    "                    \n",
    "            # Break the ratio of the model\n",
    "            try:\n",
    "                if ratio[0]/ratio[1] >= factor:\n",
    "                    break\n",
    "            except ZeroDivisionError as e:\n",
    "                pass\n",
    "\n",
    "        # If no more nodes in the queue\n",
    "        except StopIteration:\n",
    "            queue.popleft()\n",
    "\n",
    "    # Return the edges\n",
    "    return edges, ratio, visited\n",
    "\n",
    "def n_depth_search(G, frontiers, neighbors, label_dict, depth, ratio, visited, edges):\n",
    "    '''\n",
    "    Define the depth until the nodes are important\n",
    "    '''\n",
    "    # Get the nodes for the source\n",
    "    new_set = set()\n",
    "    for front_node in frontiers:\n",
    "        # Add the front node to the visited\n",
    "        visited.add(front_node)\n",
    "\n",
    "        # Maintain the ratio\n",
    "        ratio[label_dict[front_node]] += 1\n",
    "        \n",
    "        # Collect the nodes\n",
    "        neighbors_ = neighbors(front_node)\n",
    "\n",
    "        # Loop the neighbors\n",
    "        for neigh in neighbors_:\n",
    "            # Updated the visited\n",
    "            visited.add(neigh)\n",
    "\n",
    "            # Updates the edges\n",
    "            edges.append([front_node, neigh])\n",
    "\n",
    "            # Make the new frontier list\n",
    "            new_set.add(neigh)\n",
    "\n",
    "            # Add the ratio to the list\n",
    "            ratio[label_dict[neigh]] += 1\n",
    "\n",
    "    return new_set, ratio, visited, edges\n",
    "\n",
    "def frontier_sampling(frontiers, edges, neighbors, ratio, visited):\n",
    "    '''\n",
    "    Performs frontier sampling on the frontier to chose initial nodes\n",
    "    '''\n",
    "    # Define the prob of selection \n",
    "    degrees = [G.degree(i) for i in frontiers]\n",
    "    probs = [G.degree(i) / sum(degrees) for i in frontiers]\n",
    "\n",
    "    # Choose a frontier\n",
    "    chosen_frontier = np.random.choice(list(frontiers), p=probs, size=1)[0]\n",
    "\n",
    "    # Get the neighbors of the chosen frontier\n",
    "    frontier_neighbors = list(neighbors(chosen_frontier))\n",
    "\n",
    "    # Randomly select a child node\n",
    "    random_child = random.choice(frontier_neighbors)\n",
    "\n",
    "    if random_child not in visited:\n",
    "        # Add the edge\n",
    "        edges.append([chosen_frontier, random_child])\n",
    "    \n",
    "        # Update the ratio\n",
    "        ratio[label_dict[random_child]] += 1\n",
    "    \n",
    "        # Update the frontiers\n",
    "        frontiers.remove(chosen_frontier)\n",
    "        visited.add(chosen_frontier)\n",
    "        frontiers.add(random_child)\n",
    "\n",
    "    # Return frontiers\n",
    "    return frontiers, edges, ratio, visited\n",
    "\n",
    "def frontier_bfs(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100, depth=1):\n",
    "    '''\n",
    "    Uses frontiers to start and then uses breath first seach to complete the models\n",
    "    '''\n",
    "    # Define the ratio\n",
    "    ratio = {0: 0, 1: 0}\n",
    "\n",
    "    # Define the visited nodes\n",
    "    visited = set()\n",
    "\n",
    "    # Placeholder for edges\n",
    "    edges = []\n",
    "\n",
    "    # Define the frontier nodes\n",
    "    frontiers = set([source])\n",
    "\n",
    "    # Stage 1 frontiers\n",
    "    for _ in range(0, depth):\n",
    "        frontiers, ratio, visited, edges = n_depth_search(G, frontiers, neighbors,\n",
    "                                                         label_dict, 1, ratio,\n",
    "                                                         visited, edges)\n",
    "\n",
    "    # Stage 2 sampling (fronier sampling)\n",
    "    depth_val = random.randint(2, 5)\n",
    "    for _ in range(depth_val):\n",
    "        frontiers, edges, ratio, visited = frontier_sampling(frontiers, edges, neighbors, ratio, visited)\n",
    "\n",
    "    assert source not in frontiers, \"Error source in frontier after stage 2\"\n",
    "\n",
    "    # Update the visisted\n",
    "    visited.update(frontiers)\n",
    "\n",
    "    # Define random probs for the frontiers\n",
    "    frontier_probs = list(np.random.rand(len(frontiers)))\n",
    "    frontier_list = list(frontiers)\n",
    "\n",
    "    # Sort the list based on the probs\n",
    "    frontiers_with_probs_sorted = sorted([[frontier_list[i], frontier_probs[i]] for i in range(len(frontier_list))],\n",
    "                                         key=lambda x : x[-1],\n",
    "                                         reverse=True)\n",
    "    \n",
    "    # Loop over the frontiers\n",
    "    for front_node, _ in frontiers_with_probs_sorted:\n",
    "        # Run BFS on the frontier to collect its node\n",
    "        edges_bfs, ratio, visited = bfs_custom(G=G, source=front_node, label_dict=label_dict,\n",
    "                                                neighbors=neighbors, depth_limit=None,\n",
    "                                                ratio=ratio, visited=visited)\n",
    "\n",
    "        # Add to the edges\n",
    "        edges.extend(edges_bfs)\n",
    "    \n",
    "    # Return the edges\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b05b78eb-8dc5-45dc-924f-dfea515af92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_bfs_edges(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100):\n",
    "    '''\n",
    "    Simple BFS\n",
    "    '''\n",
    "    if callable(sort_neighbors):\n",
    "        _neighbors = neighbors\n",
    "        neighbors = lambda node: iter(sort_neighbors(_neighbors(node)))\n",
    "    \n",
    "    ratio = {0: 0, 1: 0} \n",
    "    visited = {source}\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    queue = deque([(source, depth_limit, neighbors(source))])\n",
    "    ratio[label_dict[source]] += 1\n",
    "    \n",
    "    edges = []\n",
    "    while queue:\n",
    "        parent, depth_now, children = queue[0]\n",
    "        try:\n",
    "            # Logic for random first seach\n",
    "            child = next(children)\n",
    "            if child not in visited:\n",
    "                edges.append((parent, child))\n",
    "                visited.add(child)\n",
    "                ratio[label_dict[child]] += 1\n",
    "                if depth_now > 1:\n",
    "                    queue.append((child, depth_now - 1, neighbors(child)))\n",
    "            try:\n",
    "                if ratio[0]/ratio[1] >= factor:\n",
    "                    return edges\n",
    "            except ZeroDivisionError as e:\n",
    "                pass\n",
    "        except StopIteration:\n",
    "            queue.popleft()\n",
    "            \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288e66fa-c747-4152-88f3-6d297781ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_mhrw_edges(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100):\n",
    "    \"\"\"\n",
    "    Iterate over edges in a MHR search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the ratio \n",
    "    ratio = {0: 0, 1: 0}\n",
    "    visited = {source}\n",
    "\n",
    "    # Define the depth limit\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    # No need for a queue\n",
    "    node_list = set([source])\n",
    "    parent_node = source\n",
    "    parent_neighbors = list(neighbors(source))\n",
    "    node_list.update(parent_neighbors)\n",
    "\n",
    "    # Find the degree of the parent node\n",
    "    degree_parent = G.degree(source)\n",
    "\n",
    "    # Maintain the ratio\n",
    "    ratio[label_dict[source]] += 1\n",
    "\n",
    "    # Placeholder for collected edges\n",
    "    edges = []\n",
    "    set_all_added_nodes = set()\n",
    "\n",
    "    # Define the logic for MHRW\n",
    "    idx = 0\n",
    "    while (ratio[0]/ratio[1]) < factor:\n",
    "        # Check if node list is not empty\n",
    "        if len(node_list) > 0:\n",
    "            # Get the next child\n",
    "            child = node_list.pop()\n",
    "    \n",
    "            # Define the probability\n",
    "            p = round(random.uniform(0, 1), 4)\n",
    "    \n",
    "            # Add nodes and update visited\n",
    "            if child not in visited and label_dict[child] != 1:\n",
    "                # Neighbors of child\n",
    "                child_neighbors = neighbors(child)\n",
    "    \n",
    "                # Collect the degree of the child\n",
    "                degree_child = G.degree(child)\n",
    "    \n",
    "                # Check if probability constraint is satisfied\n",
    "                if ((p <= min(1, degree_parent / degree_child)) and (child in list(neighbors(parent_node)))) or (idx == 0):\n",
    "                    # Update the edges and other list\n",
    "                    edges.append((parent_node, child))\n",
    "\n",
    "                    # Update the global list\n",
    "                    set_all_added_nodes.update((parent_node, child))\n",
    "\n",
    "                    # Uodate the visited and ratio\n",
    "                    visited.add(child)\n",
    "                    ratio[label_dict[child]] += 1\n",
    "                    \n",
    "                    # Replace the data\n",
    "                    parent_node = child\n",
    "                    degree_parent = degree_child\n",
    "                    node_list.clear()\n",
    "                    node_list.update(child_neighbors)\n",
    "                    idx += 1\n",
    "        else:\n",
    "            # Update the nodelist with random nodes\n",
    "            node_list.update(set(random.sample(list(set(G.nodes()) - set_all_added_nodes), 3)))\n",
    "            \n",
    "            # Get the new parent\n",
    "            parent_node = node_list.pop()\n",
    "            \n",
    "            # Refresh the node list\n",
    "            degree_parent = G.degree(parent_node)\n",
    "            node_list.clear()\n",
    "            node_list.update(list(neighbors(parent_node)))\n",
    "\n",
    "    # Return the data\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ca225f7-6bce-4869-ad9e-efb50f1d75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_frontier_edges(G, source, label_dict, neighbors=None, depth_limit=None,\n",
    "                           sort_neighbors=None, factor=100, num_frontiers=20):\n",
    "    \"\"\"\n",
    "    Iterate over edges in a MHR search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the ratio \n",
    "    ratio = {0: 0, 1: 0}\n",
    "    visited = {source}\n",
    "    break_condition = False\n",
    "\n",
    "    # Define the depth limit\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    # Define the frontier nodes\n",
    "    frontier_nodes = set()\n",
    "    neighbors_s_node = set(neighbors(source))\n",
    "\n",
    "    # Find the reachable frontier nodes\n",
    "    reachable_nodes = list(nx.single_source_shortest_path_length(G, source).keys())\n",
    "    reachable_nodes.remove(source)\n",
    "\n",
    "    # Add some random nodes just in case\n",
    "    temp_ = list(np.random.choice(list(G.nodes), size=num_frontiers))\n",
    "    reachable_nodes.extend(temp_)\n",
    "    reachable_nodes = [i for i in reachable_nodes if i in label_dict][:num_frontiers]\n",
    "\n",
    "    # Update the frontier nodes\n",
    "    frontier_nodes.update(reachable_nodes)\n",
    "    \n",
    "    # Maintain the ratio\n",
    "    ratio[label_dict[source]] += 1\n",
    "\n",
    "    # Placeholder for collected edges\n",
    "    edges = [(source, i) for i in frontier_nodes]\n",
    "\n",
    "    # Add the seed nodes to the mix\n",
    "    for node in frontier_nodes:\n",
    "        ratio[label_dict[node]] += 1\n",
    "\n",
    "    # Define the patience\n",
    "    patience = 0\n",
    "\n",
    "    # Loop and collect edgeneric_frontier_edges\n",
    "    while (ratio[0]/ratio[1]) < factor:\n",
    "        # Select a new node with some probability\n",
    "        degrees = [G.degree(i) for i in frontier_nodes]\n",
    "        list_probs = [G.degree(i) / sum(degrees) for i in frontier_nodes]\n",
    "        \n",
    "        # Get the next child\n",
    "        chosen_frontier = np.random.choice(list(frontier_nodes), p=list_probs, size=1)[0]\n",
    "\n",
    "        # Neighbors of child\n",
    "        frontier_neighbors = list(neighbors(chosen_frontier))\n",
    "\n",
    "        # Randomly chose one of them\n",
    "        random_child = random.choice(frontier_neighbors)\n",
    "\n",
    "        # Add nodes and update visited\n",
    "        if random_child not in visited and label_dict[random_child] != 1:\n",
    "            # Update the edges and other list\n",
    "            edges.append((chosen_frontier, random_child))\n",
    "            ratio[label_dict[random_child]] += 1\n",
    "\n",
    "            # Replace u by v in the node list\n",
    "            frontier_nodes.remove(chosen_frontier)\n",
    "            frontier_nodes.add(random_child)\n",
    "\n",
    "            # Reset patience\n",
    "            patience = 0\n",
    "        else:\n",
    "            # Increment the patience\n",
    "            patience += 1\n",
    "\n",
    "            if break_condition:\n",
    "                break\n",
    "            \n",
    "            # Edge case for unreacheable nodes\n",
    "            if patience == 50:\n",
    "                frontier_nodes = visited - set([source])\n",
    "                break_condition = True\n",
    "            \n",
    "    # Return the data\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e59ad080-8b60-4461-aa60-54da964f9881",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "c2d8eed3-fe7f-4ad8-8e36-d0f435be37d6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def load_local_data_store(data_dir:str) -> Store:   \n",
    "    # build the store\n",
    "    store = Store(\n",
    "        base_dir=data_dir,\n",
    "        protocol='file'\n",
    "    )\n",
    "    return store\n",
    "\n",
    "datastore = load_local_data_store(data_dir)\n",
    "df_p = pd.read_parquet(\n",
    "        datastore.open_file(partitions_dir)\n",
    "    ).reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b5aad7f-a19d-4e0d-b109-018e35370c7e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a7059ad3-9ed8-41c4-931a-e9f39fa5e580",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "counters = {}\n",
    "graph_data = {}\n",
    "labelled = {}\n",
    "others = {}\n",
    "for sp, A in df_p.groupby('split'):\n",
    "    graph_data[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    labelled[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    counters[sp] = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba361df",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "5301c583-2ae8-481e-ba97-67ab09de6c03",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Scaler...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a5919bc1a64030b163c64c13692470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit_scaler(graph_data: Dict):\n",
    "    print(\"Fitting Scaler...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    for p in tqdm(graph_data):\n",
    "        df_f = pd.read_parquet(f\"./data/dataset/cache/features/features_{p}.parquet\")\n",
    "        X = df_f[FEATURE_COLUMNS].fillna(value=0.0).values\n",
    "        scaler.partial_fit(X)\n",
    "        del df_f, X\n",
    "        gc.collect()\n",
    "    return scaler\n",
    "\n",
    "scaler = fit_scaler(graph_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0783346-6bff-43f5-b25a-2e6a3def15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_graph(result, network, labels, features, idx, name_sampling):\n",
    "    # Convert the FS seach to dataframe from merging and stuff\n",
    "    df_result = pd.DataFrame(result, columns=['from', 'to'])\n",
    "    \n",
    "    # Undirected to directed\n",
    "    directed1 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['from', 'to'])\n",
    "    directed2 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['to', 'from'])[['from_x', 'to_x', 'partition']].rename(columns={\"from_x\": \"from\", \"to_x\": \"to\", 'partition': 'partition'})\n",
    "    samples = pd.concat([directed1, directed2], axis=0)\n",
    "    edges_list = samples[['from', 'to']].values\n",
    "\n",
    "    # Get the unique graph edges\n",
    "    df_node = pd.DataFrame(set(edges_list.reshape(-1)), columns=['node'])\n",
    "\n",
    "    # Process the features and labels\n",
    "    sample_labels = labels.merge(df_node, how='inner').sort_values(by=['node'])\n",
    "    sample_features = features.merge(sample_labels, how='inner', on='txid').sort_values(by=['node']).drop(['node'], axis=1)\n",
    "    mapping = dict(zip(sample_labels.node.values, range(len(sample_labels))))\n",
    "    samples[['from', 'to']] = samples[['from', 'to']].replace(mapping)\n",
    "    sample_labels.node = [i for i in range(len(sample_labels))]\n",
    "    sample_features[FEATURE_COLUMNS] = scaler.transform(sample_features[FEATURE_COLUMNS].values)\n",
    "\n",
    "    # Save the data\n",
    "    samples.to_parquet(f'./sampling_comparison/{name_sampling}_edges_{idx}.parquet')\n",
    "    sample_labels.to_parquet(f'./sampling_comparison/{name_sampling}_labels_{idx}.parquet')\n",
    "    sample_features.to_parquet(f'./sampling_comparison/{name_sampling}_features_{idx}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398b0b1-5761-42f0-88b4-dc3db6ebb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New parition file\n",
    "p_dict = {'index': [], 'split': []}\n",
    "\n",
    "# Placeholder\n",
    "idx = 0\n",
    "\n",
    "# Loop over splits\n",
    "for sp in splits:\n",
    "    # Loop over split data\n",
    "    bar = tqdm(graph_data[sp], total=len(graph_data[sp]))\n",
    "    for p in bar:\n",
    "        \n",
    "        # Load the required files\n",
    "        network = pd.read_parquet(f'./{data_dir}/cache/edges/edges_{p}.parquet')\n",
    "        labels = pd.read_parquet(f'./{data_dir}/labels/labels_{p}.parquet')\n",
    "        features = pd.read_parquet(f'./{data_dir}/cache/features/features_{p}.parquet')\n",
    "    \n",
    "        # Convert to network x graph\n",
    "        G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "        successors = G.neighbors # can retrieve all neighbors of a particular node with []\n",
    "        \n",
    "        # Replace all label 2 as label 0\n",
    "        labels.loc[labels['label'] == 2, 'label'] = 0\n",
    "        label_dict = dict(zip(labels.node, labels.label))\n",
    "    \n",
    "        # Construct graph from each positive node\n",
    "        for _, pos_node in enumerate(labels[labels['label']==1].node.values):\n",
    "            try: # the node in the label parquet may not exist in the edge parquet\n",
    "                result_bfs = generic_bfs_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "                result_bf = frontier_bfs(G, pos_node, label_dict, successors, factor=factor)\n",
    "                result_mhrw = generic_mhrw_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "                result_fs = generic_frontier_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            # Save the samples together\n",
    "            post_process_graph(result_bfs, network, labels, features, idx, \"bfs\")\n",
    "            post_process_graph(result_bf, network, labels, features, idx, \"bf\")\n",
    "            post_process_graph(result_mhrw, network, labels, features, idx, \"mhrw\")\n",
    "            post_process_graph(result_fs, network, labels, features, idx, \"fs\")\n",
    "\n",
    "            # Increment the idx\n",
    "            bar.set_description(f\"File saved : {idx}\")\n",
    "            idx += 1"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
