{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f064138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "factor = 150\n",
    "data_dir = \"./data/dataset\"\n",
    "save_path = \"data/datasetBF\"\n",
    "partitions_dir = \"partitions.parquet\"\n",
    "splits = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bc5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1288a533-2faf-48a2-8b47-ca1e99676cb7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "ec54a8d6-b0cd-458d-9798-270a8a81f645",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import deque\n",
    "from pyvis.network import Network\n",
    "from lib.store import Store\n",
    "from tqdm.notebook import tqdm\n",
    "import fsspec\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "\n",
    "from lib.naming_corrections import (\n",
    "    FEATURE_COLUMNS_OTHERS,\n",
    "    FEATURES_NAMES_FROM_NEW_CACHE,\n",
    "    FEATURES_NAMES_FROM_PRELOADED_CACHE,\n",
    "    TABLES_COLUMNS_DEFAULT_LEGACY,\n",
    "    TABLES_V5_2_V4_RENAME_LEGACY,\n",
    ")\n",
    "\n",
    "FEATURE_COLUMNS = FEATURES_NAMES_FROM_PRELOADED_CACHE + FEATURE_COLUMNS_OTHERS\n",
    "TABLES_V5_2_V4_RENAME = TABLES_V5_2_V4_RENAME_LEGACY\n",
    "TABLES_COLUMNS_DEFAULT = TABLES_COLUMNS_DEFAULT_LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aa2a563-8567-472c-9086-35a620caa79d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a348c79b-3202-4ef9-8e49-872d78a5b97d",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def bfs_custom(G, source, label_dict, neighbors=None, depth_limit=None, ratio=None, visited=None):\n",
    "    '''\n",
    "    Generic breath first seach algorithm\n",
    "    '''\n",
    "    # Define the depth of the search\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "        \n",
    "    # neighbors(source): return a generator of source's neighbors\n",
    "    queue = deque([(source, depth_limit, neighbors(source))])\n",
    "\n",
    "    # Placeholder for edges\n",
    "    edges = []\n",
    "\n",
    "    # Run the queue\n",
    "    while queue:\n",
    "        parent, depth_now, children = queue[0]\n",
    "        try:\n",
    "            # Logic for random first seach\n",
    "            child = next(children)\n",
    "\n",
    "            # If the child has not been visited before\n",
    "            if child not in visited:\n",
    "                # Construct an edge\n",
    "                edges.append((parent, child))\n",
    "\n",
    "                # Update visited\n",
    "                visited.add(child)\n",
    "\n",
    "                # Increment the ratio\n",
    "                ratio[label_dict[child]] += 1\n",
    "\n",
    "                # Incremen the child\n",
    "                if depth_now > 1:\n",
    "                    queue.append((child, depth_now - 1, neighbors(child)))\n",
    "                    \n",
    "            # Break the ratio of the model\n",
    "            try:\n",
    "                if ratio[0]/ratio[1] >= factor:\n",
    "                    break\n",
    "            except ZeroDivisionError as e:\n",
    "                pass\n",
    "\n",
    "        # If no more nodes in the queue\n",
    "        except StopIteration:\n",
    "            queue.popleft()\n",
    "\n",
    "    # Return the edges\n",
    "    return edges, ratio, visited\n",
    "\n",
    "def n_depth_search(G, frontiers, neighbors, label_dict, depth, ratio, visited, edges):\n",
    "    '''\n",
    "    Define the depth until the nodes are important\n",
    "    '''\n",
    "    # Get the nodes for the source\n",
    "    new_set = set()\n",
    "    for front_node in frontiers:\n",
    "        # Add the front node to the visited\n",
    "        visited.add(front_node)\n",
    "\n",
    "        # Maintain the ratio\n",
    "        ratio[label_dict[front_node]] += 1\n",
    "        \n",
    "        # Collect the nodes\n",
    "        neighbors_ = neighbors(front_node)\n",
    "\n",
    "        # Loop the neighbors\n",
    "        for neigh in neighbors_:\n",
    "            # Updated the visited\n",
    "            visited.add(neigh)\n",
    "\n",
    "            # Updates the edges\n",
    "            edges.append([front_node, neigh])\n",
    "\n",
    "            # Make the new frontier list\n",
    "            new_set.add(neigh)\n",
    "\n",
    "            # Add the ratio to the list\n",
    "            ratio[label_dict[neigh]] += 1\n",
    "\n",
    "    return new_set, ratio, visited, edges\n",
    "\n",
    "def frontier_sampling(frontiers, edges, neighbors, ratio, visited):\n",
    "    '''\n",
    "    Performs frontier sampling on the frontier to chose initial nodes\n",
    "    '''\n",
    "    # Define the prob of selection \n",
    "    degrees = [G.degree(i) for i in frontiers]\n",
    "    probs = [G.degree(i) / sum(degrees) for i in frontiers]\n",
    "\n",
    "    # Choose a frontier\n",
    "    chosen_frontier = np.random.choice(list(frontiers), p=probs, size=1)[0]\n",
    "\n",
    "    # Get the neighbors of the chosen frontier\n",
    "    frontier_neighbors = list(neighbors(chosen_frontier))\n",
    "\n",
    "    # Randomly select a child node\n",
    "    random_child = random.choice(frontier_neighbors)\n",
    "\n",
    "    if random_child not in visited:\n",
    "        # Add the edge\n",
    "        edges.append([chosen_frontier, random_child])\n",
    "    \n",
    "        # Update the ratio\n",
    "        ratio[label_dict[random_child]] += 1\n",
    "    \n",
    "        # Update the frontiers\n",
    "        frontiers.remove(chosen_frontier)\n",
    "        visited.add(chosen_frontier)\n",
    "        frontiers.add(random_child)\n",
    "\n",
    "    # Return frontiers\n",
    "    return frontiers, edges, ratio, visited\n",
    "\n",
    "def frontier_bfs(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100, depth=1):\n",
    "    '''\n",
    "    Uses frontiers to start and then uses breath first seach to complete the models\n",
    "    '''\n",
    "    # Define the ratio\n",
    "    ratio = {0: 0, 1: 0}\n",
    "\n",
    "    # Define the visited nodes\n",
    "    visited = set()\n",
    "\n",
    "    # Placeholder for edges\n",
    "    edges = []\n",
    "\n",
    "    # Define the frontier nodes\n",
    "    frontiers = set([source])\n",
    "\n",
    "    # Stage 1 frontiers\n",
    "    for _ in range(0, depth):\n",
    "        frontiers, ratio, visited, edges = n_depth_search(G, frontiers, neighbors,\n",
    "                                                         label_dict, 1, ratio,\n",
    "                                                         visited, edges)\n",
    "\n",
    "    # Stage 2 sampling (fronier sampling)\n",
    "    depth_val = random.randint(2, 5)\n",
    "    for _ in range(depth_val):\n",
    "        frontiers, edges, ratio, visited = frontier_sampling(frontiers, edges, neighbors, ratio, visited)\n",
    "\n",
    "    assert source not in frontiers, \"Error source in frontier after stage 2\"\n",
    "\n",
    "    # Update the visisted\n",
    "    visited.update(frontiers)\n",
    "\n",
    "    # Define random probs for the frontiers\n",
    "    frontier_probs = list(np.random.rand(len(frontiers)))\n",
    "    frontier_list = list(frontiers)\n",
    "\n",
    "    # Sort the list based on the probs\n",
    "    frontiers_with_probs_sorted = sorted([[frontier_list[i], frontier_probs[i]] for i in range(len(frontier_list))],\n",
    "                                         key=lambda x : x[-1],\n",
    "                                         reverse=True)\n",
    "    \n",
    "    # Loop over the frontiers\n",
    "    for front_node, _ in frontiers_with_probs_sorted:\n",
    "        # Run BFS on the frontier to collect its node\n",
    "        edges_bfs, ratio, visited = bfs_custom(G=G, source=front_node, label_dict=label_dict,\n",
    "                                                neighbors=neighbors, depth_limit=None,\n",
    "                                                ratio=ratio, visited=visited)\n",
    "\n",
    "        # Add to the edges\n",
    "        edges.extend(edges_bfs)\n",
    "    \n",
    "    # Return the edges\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b05b78eb-8dc5-45dc-924f-dfea515af92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_bfs_edges(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100):\n",
    "    '''\n",
    "    Simple BFS\n",
    "    '''\n",
    "    if callable(sort_neighbors):\n",
    "        _neighbors = neighbors\n",
    "        neighbors = lambda node: iter(sort_neighbors(_neighbors(node)))\n",
    "    \n",
    "    ratio = {0: 0, 1: 0} \n",
    "    visited = {source}\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    queue = deque([(source, depth_limit, neighbors(source))])\n",
    "    ratio[label_dict[source]] += 1\n",
    "    \n",
    "    edges = []\n",
    "    while queue:\n",
    "        parent, depth_now, children = queue[0]\n",
    "        try:\n",
    "            # Logic for random first seach\n",
    "            child = next(children)\n",
    "            if child not in visited:\n",
    "                edges.append((parent, child))\n",
    "                visited.add(child)\n",
    "                ratio[label_dict[child]] += 1\n",
    "                if depth_now > 1:\n",
    "                    queue.append((child, depth_now - 1, neighbors(child)))\n",
    "            try:\n",
    "                if ratio[0]/ratio[1] >= factor:\n",
    "                    return edges\n",
    "            except ZeroDivisionError as e:\n",
    "                pass\n",
    "        except StopIteration:\n",
    "            queue.popleft()\n",
    "            \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288e66fa-c747-4152-88f3-6d297781ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_mhrw_edges(G, source, label_dict, neighbors=None, depth_limit=None, sort_neighbors=None, factor=100):\n",
    "    \"\"\"\n",
    "    Iterate over edges in a MHR search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the ratio \n",
    "    ratio = {0: 0, 1: 0}\n",
    "    visited = {source}\n",
    "\n",
    "    # Define the depth limit\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    # No need for a queue\n",
    "    node_list = set([source])\n",
    "    parent_node = source\n",
    "    parent_neighbors = list(neighbors(source))\n",
    "    node_list.update(parent_neighbors)\n",
    "\n",
    "    # Find the degree of the parent node\n",
    "    degree_parent = G.degree(source)\n",
    "\n",
    "    # Maintain the ratio\n",
    "    ratio[label_dict[source]] += 1\n",
    "\n",
    "    # Placeholder for collected edges\n",
    "    edges = []\n",
    "    set_all_added_nodes = set()\n",
    "\n",
    "    # Define the logic for MHRW\n",
    "    idx = 0\n",
    "    while (ratio[0]/ratio[1]) < factor:\n",
    "        # Check if node list is not empty\n",
    "        if len(node_list) > 0:\n",
    "            # Get the next child\n",
    "            child = node_list.pop()\n",
    "    \n",
    "            # Define the probability\n",
    "            p = round(random.uniform(0, 1), 4)\n",
    "    \n",
    "            # Add nodes and update visited\n",
    "            if child not in visited and label_dict[child] != 1:\n",
    "                # Neighbors of child\n",
    "                child_neighbors = neighbors(child)\n",
    "    \n",
    "                # Collect the degree of the child\n",
    "                degree_child = G.degree(child)\n",
    "    \n",
    "                # Check if probability constraint is satisfied\n",
    "                if ((p <= min(1, degree_parent / degree_child)) and (child in list(neighbors(parent_node)))) or (idx == 0):\n",
    "                    # Update the edges and other list\n",
    "                    edges.append((parent_node, child))\n",
    "\n",
    "                    # Update the global list\n",
    "                    set_all_added_nodes.update((parent_node, child))\n",
    "\n",
    "                    # Uodate the visited and ratio\n",
    "                    visited.add(child)\n",
    "                    ratio[label_dict[child]] += 1\n",
    "                    \n",
    "                    # Replace the data\n",
    "                    parent_node = child\n",
    "                    degree_parent = degree_child\n",
    "                    node_list.clear()\n",
    "                    node_list.update(child_neighbors)\n",
    "                    idx += 1\n",
    "        else:\n",
    "            # Update the nodelist with random nodes\n",
    "            node_list.update(set(random.sample(list(set(G.nodes()) - set_all_added_nodes), 3)))\n",
    "            \n",
    "            # Get the new parent\n",
    "            parent_node = node_list.pop()\n",
    "            \n",
    "            # Refresh the node list\n",
    "            degree_parent = G.degree(parent_node)\n",
    "            node_list.clear()\n",
    "            node_list.update(list(neighbors(parent_node)))\n",
    "\n",
    "    # Return the data\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ca225f7-6bce-4869-ad9e-efb50f1d75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_frontier_edges(G, source, label_dict, neighbors=None, depth_limit=None,\n",
    "                           sort_neighbors=None, factor=100, num_frontiers=20):\n",
    "    \"\"\"\n",
    "    Iterate over edges in a MHR search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the ratio \n",
    "    ratio = {0: 0, 1: 0}\n",
    "    visited = {source}\n",
    "    break_condition = False\n",
    "\n",
    "    # Define the depth limit\n",
    "    if depth_limit is None:\n",
    "        depth_limit = len(G)\n",
    "\n",
    "    # Define the frontier nodes\n",
    "    frontier_nodes = set()\n",
    "    neighbors_s_node = set(neighbors(source))\n",
    "\n",
    "    # Find the reachable frontier nodes\n",
    "    reachable_nodes = list(nx.single_source_shortest_path_length(G, source).keys())\n",
    "    reachable_nodes.remove(source)\n",
    "\n",
    "    # Add some random nodes just in case\n",
    "    temp_ = list(np.random.choice(list(G.nodes), size=num_frontiers))\n",
    "    reachable_nodes.extend(temp_)\n",
    "    reachable_nodes = [i for i in reachable_nodes if i in label_dict][:num_frontiers]\n",
    "\n",
    "    # Update the frontier nodes\n",
    "    frontier_nodes.update(reachable_nodes)\n",
    "    \n",
    "    # Maintain the ratio\n",
    "    ratio[label_dict[source]] += 1\n",
    "\n",
    "    # Placeholder for collected edges\n",
    "    edges = [(source, i) for i in frontier_nodes]\n",
    "\n",
    "    # Add the seed nodes to the mix\n",
    "    for node in frontier_nodes:\n",
    "        ratio[label_dict[node]] += 1\n",
    "\n",
    "    # Define the patience\n",
    "    patience = 0\n",
    "\n",
    "    # Loop and collect edgeneric_frontier_edges\n",
    "    while (ratio[0]/ratio[1]) < factor:\n",
    "        # Select a new node with some probability\n",
    "        degrees = [G.degree(i) for i in frontier_nodes]\n",
    "        list_probs = [G.degree(i) / sum(degrees) for i in frontier_nodes]\n",
    "        \n",
    "        # Get the next child\n",
    "        chosen_frontier = np.random.choice(list(frontier_nodes), p=list_probs, size=1)[0]\n",
    "\n",
    "        # Neighbors of child\n",
    "        frontier_neighbors = list(neighbors(chosen_frontier))\n",
    "\n",
    "        # Randomly chose one of them\n",
    "        random_child = random.choice(frontier_neighbors)\n",
    "\n",
    "        # Add nodes and update visited\n",
    "        if random_child not in visited and label_dict[random_child] != 1:\n",
    "            # Update the edges and other list\n",
    "            edges.append((chosen_frontier, random_child))\n",
    "            ratio[label_dict[random_child]] += 1\n",
    "\n",
    "            # Replace u by v in the node list\n",
    "            frontier_nodes.remove(chosen_frontier)\n",
    "            frontier_nodes.add(random_child)\n",
    "\n",
    "            # Reset patience\n",
    "            patience = 0\n",
    "        else:\n",
    "            # Increment the patience\n",
    "            patience += 1\n",
    "\n",
    "            if break_condition:\n",
    "                break\n",
    "            \n",
    "            # Edge case for unreacheable nodes\n",
    "            if patience == 50:\n",
    "                frontier_nodes = visited - set([source])\n",
    "                break_condition = True\n",
    "            \n",
    "    # Return the data\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e59ad080-8b60-4461-aa60-54da964f9881",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "c2d8eed3-fe7f-4ad8-8e36-d0f435be37d6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "def load_local_data_store(data_dir:str) -> Store:   \n",
    "    # build the store\n",
    "    store = Store(\n",
    "        base_dir=data_dir,\n",
    "        protocol='file'\n",
    "    )\n",
    "    return store\n",
    "\n",
    "datastore = load_local_data_store(data_dir)\n",
    "df_p = pd.read_parquet(\n",
    "        datastore.open_file(partitions_dir)\n",
    "    ).reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b5aad7f-a19d-4e0d-b109-018e35370c7e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "a7059ad3-9ed8-41c4-931a-e9f39fa5e580",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "counters = {}\n",
    "graph_data = {}\n",
    "labelled = {}\n",
    "others = {}\n",
    "for sp, A in df_p.groupby('split'):\n",
    "    graph_data[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    labelled[sp] = {\n",
    "        x: None\n",
    "        for x in sorted(A['index'])\n",
    "    }\n",
    "    counters[sp] = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba361df",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "5301c583-2ae8-481e-ba97-67ab09de6c03",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Scaler...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a5919bc1a64030b163c64c13692470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit_scaler(graph_data: Dict):\n",
    "    print(\"Fitting Scaler...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    for p in tqdm(graph_data):\n",
    "        df_f = pd.read_parquet(f\"./data/dataset/cache/features/features_{p}.parquet\")\n",
    "        X = df_f[FEATURE_COLUMNS].fillna(value=0.0).values\n",
    "        scaler.partial_fit(X)\n",
    "        del df_f, X\n",
    "        gc.collect()\n",
    "    return scaler\n",
    "\n",
    "scaler = fit_scaler(graph_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0783346-6bff-43f5-b25a-2e6a3def15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_graph(result, network, labels, features, idx, name_sampling):\n",
    "    # Convert the FS seach to dataframe from merging and stuff\n",
    "    df_result = pd.DataFrame(result, columns=['from', 'to'])\n",
    "    \n",
    "    # Undirected to directed\n",
    "    directed1 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['from', 'to'])\n",
    "    directed2 = network.merge(df_result, how='inner', left_on=['from', 'to'], right_on=['to', 'from'])[['from_x', 'to_x', 'partition']].rename(columns={\"from_x\": \"from\", \"to_x\": \"to\", 'partition': 'partition'})\n",
    "    samples = pd.concat([directed1, directed2], axis=0)\n",
    "    edges_list = samples[['from', 'to']].values\n",
    "\n",
    "    # Get the unique graph edges\n",
    "    df_node = pd.DataFrame(set(edges_list.reshape(-1)), columns=['node'])\n",
    "\n",
    "    # Process the features and labels\n",
    "    sample_labels = labels.merge(df_node, how='inner').sort_values(by=['node'])\n",
    "    sample_features = features.merge(sample_labels, how='inner', on='txid').sort_values(by=['node']).drop(['node'], axis=1)\n",
    "    mapping = dict(zip(sample_labels.node.values, range(len(sample_labels))))\n",
    "    samples[['from', 'to']] = samples[['from', 'to']].replace(mapping)\n",
    "    sample_labels.node = [i for i in range(len(sample_labels))]\n",
    "    sample_features[FEATURE_COLUMNS] = scaler.transform(sample_features[FEATURE_COLUMNS].values)\n",
    "\n",
    "    # Save the data\n",
    "    samples.to_parquet(f'./sampling_comparison/{name_sampling}_edges_{idx}.parquet')\n",
    "    sample_labels.to_parquet(f'./sampling_comparison/{name_sampling}_labels_{idx}.parquet')\n",
    "    sample_features.to_parquet(f'./sampling_comparison/{name_sampling}_features_{idx}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7398b0b1-5761-42f0-88b4-dc3db6ebb3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6cdf3d20d348de8b43c12fa164a53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7490, 0.390408064784192], [7487, 0.15388339440543775]]\n",
      "[[3937, 0.12257089530959497]]\n",
      "[[24806, 0.5117702517678953], [14188, 0.45293267867024], [93978, 0.0021971178125413937]]\n",
      "[[62538, 0.809696187013152], [61456, 0.2711873162973757]]\n",
      "[[95803, 0.19319611212477028], [89158, 0.17130706679801355]]\n",
      "[[42595, 0.8208524411942988], [43750, 0.5588573338655894]]\n",
      "[[45698, 0.9696252287600801], [80999, 0.38463761897670634]]\n",
      "[[8842, 0.9111472998104647], [12447, 0.26250234562139774]]\n",
      "[[82561, 0.8936573228057618], [82581, 0.18201330946386685]]\n",
      "[[68993, 0.7211278976933982], [57144, 0.07941631567692331]]\n",
      "[[84471, 0.18041576945383742]]\n",
      "[[20415, 0.7200954138395627], [15032, 0.4402247153037092], [3276, 0.3208041570046397], [39525, 0.27404642966844783], [17727, 0.10632137138137021], [10861, 0.09287175015632021], [27497, 0.024728069654555318]]\n",
      "[[48759, 0.9722293125318281], [76161, 0.6796119200618356]]\n",
      "[[89298, 0.7018313946711479]]\n",
      "[[44199, 0.4098546807612493], [41593, 0.2252678937190118]]\n",
      "[[27996, 0.7416837483167561]]\n",
      "[[94291, 0.8326869291022461], [94297, 0.3559370833702328]]\n",
      "[[82553, 0.6562124612441274]]\n",
      "[[70330, 0.32638647685372757]]\n",
      "[[25109, 0.6769135418411815], [13694, 0.17039572314433427]]\n",
      "[[55552, 0.7976884982583141], [57205, 0.054136151316906855], [47045, 0.037056339224668555]]\n",
      "[[47504, 0.5136305624999215], [47506, 0.15962396707089488]]\n",
      "[[41507, 0.9726706622726059], [41511, 0.8201755780456861]]\n",
      "[[34568, 0.5922517805371607]]\n",
      "[[34478, 0.8977304138255233], [21702, 0.2129710641968957]]\n",
      "[[38376, 0.8692417319892298], [41245, 0.8453008285507599]]\n",
      "[[67735, 0.9340801926378799]]\n",
      "[[69815, 0.7534745917424055], [72687, 0.21351199878121807]]\n",
      "[[97354, 0.5723810490502388]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save the samples together\u001b[39;00m\n\u001b[1;32m     37\u001b[0m post_process_graph(result_bfs, network, labels, features, idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mpost_process_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_bf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m post_process_graph(result_mhrw, network, labels, features, idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmhrw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m post_process_graph(result_fs, network, labels, features, idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mpost_process_graph\u001b[0;34m(result, network, labels, features, idx, name_sampling)\u001b[0m\n\u001b[1;32m     23\u001b[0m samples\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./sampling_comparison/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_sampling\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_edges_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m sample_labels\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./sampling_comparison/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_sampling\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_labels_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43msample_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./sampling_comparison/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname_sampling\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_features_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/core/frame.py:2836\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2752\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2832\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2833\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2834\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2837\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/io/parquet.py:420\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    418\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 420\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.10/site-packages/pandas/io/parquet.py:195\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    187\u001b[0m             table,\n\u001b[1;32m    188\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Compiler/gcc9/arrow/9.0.0/lib/python3.10/site-packages/pyarrow-9.0.0-py3.10-linux-x86_64.egg/pyarrow/parquet/__init__.py:2941\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2919\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2920\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   2921\u001b[0m             where, table\u001b[38;5;241m.\u001b[39mschema,\n\u001b[1;32m   2922\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2939\u001b[0m             dictionary_pagesize_limit\u001b[38;5;241m=\u001b[39mdictionary_pagesize_limit,\n\u001b[1;32m   2940\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m-> 2941\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2942\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(where):\n",
      "File \u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Compiler/gcc9/arrow/9.0.0/lib/python3.10/site-packages/pyarrow-9.0.0-py3.10-linux-x86_64.egg/pyarrow/parquet/__init__.py:1017\u001b[0m, in \u001b[0;36mParquetWriter.write_table\u001b[0;34m(self, table, row_group_size)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable schema does not match schema used to create file: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1013\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1014\u001b[0m            \u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m.\u001b[39mschema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema))\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 1017\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# New parition file\n",
    "p_dict = {'index': [], 'split': []}\n",
    "\n",
    "# Placeholder\n",
    "idx = 0\n",
    "\n",
    "# Loop over splits\n",
    "for sp in splits:\n",
    "    # Loop over split data\n",
    "    bar = tqdm(graph_data[sp], total=len(graph_data[sp]))\n",
    "    for p in bar:\n",
    "        \n",
    "        # Load the required files\n",
    "        network = pd.read_parquet(f'./{data_dir}/cache/edges/edges_{p}.parquet')\n",
    "        labels = pd.read_parquet(f'./{data_dir}/labels/labels_{p}.parquet')\n",
    "        features = pd.read_parquet(f'./{data_dir}/cache/features/features_{p}.parquet')\n",
    "    \n",
    "        # Convert to network x graph\n",
    "        G = nx.from_pandas_edgelist(network, 'from', 'to')\n",
    "        successors = G.neighbors # can retrieve all neighbors of a particular node with []\n",
    "        \n",
    "        # Replace all label 2 as label 0\n",
    "        labels.loc[labels['label'] == 2, 'label'] = 0\n",
    "        label_dict = dict(zip(labels.node, labels.label))\n",
    "    \n",
    "        # Construct graph from each positive node\n",
    "        for _, pos_node in enumerate(labels[labels['label']==1].node.values):\n",
    "            try: # the node in the label parquet may not exist in the edge parquet\n",
    "                result_bfs = generic_bfs_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "                result_bf = frontier_bfs(G, pos_node, label_dict, successors, factor=factor)\n",
    "                result_mhrw = generic_mhrw_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "                result_fs = generic_frontier_edges(G, pos_node, label_dict, successors, factor=factor)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            # Save the samples together\n",
    "            post_process_graph(result_bfs, network, labels, features, idx, \"bfs\")\n",
    "            post_process_graph(result_bf, network, labels, features, idx, \"bf\")\n",
    "            post_process_graph(result_mhrw, network, labels, features, idx, \"mhrw\")\n",
    "            post_process_graph(result_fs, network, labels, features, idx, \"fs\")\n",
    "\n",
    "            # Increment the idx\n",
    "            bar.set_description(f\"File saved : {idx}\")\n",
    "            idx += 1"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
